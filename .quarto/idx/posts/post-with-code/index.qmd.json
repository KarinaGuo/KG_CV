{"title":"Generating and Analysing My Massive ML Data","markdown":{"yaml":{"title":"Generating and Analysing My Massive ML Data","date":"2023-04-24","categories":["code","project"],"image":"image.jpg"},"headingText":"Using Machine Learning to Link Climate, Phylogeny and Leaf Area in Eucalypts Through a 50-fold Expansion of Current Leaf Trait Datasets","containsRefs":false,"markdown":"\n\n\n> To look at the source code, feel free to check out my [GitHub](https://github.com/KarinaGuo/Machine_Learning_in_Computer_Vision_for_Leaf_Traits)\n\nMy project involved three key parts:\n\n1.  Prepare my data for training, validating, and testing my machine learning models\n2.  My two machine learning models were then created with the datasets\n3.  I used my machine learning models to make my dataset which was analysed for trends\n\n![Figure 1. Workflow of project](workflow.jpg){fig-alt=\"Workflow of project\" fig-align=\"center\"}\n\n<p>\n\n \n\n<p>\n\n#### Generating my dataset for my first model\n\nTo build a good machine learning model, I needed a well-annotated training dataset to train, optimise and evaluate my model. In my case, this was done through *manual annotation* of herbarium sheets (archived pressed plant images). Manual annotation is essentially me telling the machine learning model what to 'learn', and looked like spending **hours** tracing the outlines of leaves on these sheets.\n\nThe annotated datasets were then divided into three sets: the *training* set, the *validating* set, and the *testing* set.\n\n-   Training Set: Used to train the model by showing it what pixels represented a leaf on a herbarium sheet.\n\n-   Validating Set: Employed to optimise the model's hyperparameters and ensure proper it generalises well on unseen data.\n\n-   Testing Set: Used to evaluate the model's performance and accuracy\n\n    <p>\n\n     \n\n    <p>\n\n#### The first model, a leaf segmentation model\n\nThis first model I created was aimed to *segment* leaves from herbarium sheets; to create *masks* of the pixels that represent each individual leaf, as in This process involved a cycle of optimisation. First, feed in the training and validating dataset with initial *training parameters*. These parameters shaped how the model learnt. Then, evaluate the quality of the first iteration of the model.\n\nThis step involved both qualitative and quantitative metrics. Qualitative metrics were looking at all the predicted herbarium sheets, like in Figure 2, and judging how it went. Did it do poorly on small leaves? Worse on broken leaves? And, so forth. Using these metrics I judged what needed to be amended in the training dataset for the next iteration. Similarly, quantitative metrics involved calculating values such as *Precision*, *Recall*, and *F1-score* which are measures of true positives, false negatives, etc. that are standardised in the machine learning field. From these metrics, I then changed the training dataset the model needed to improve it for the next iteration.\n\nOnce I've done more painstaking labelling of the training dataset, and tweaked the training parameters, I embarked the model on the second iteration. Training a brand new model and conducting the evaluation metrics again. I kept repeating this cycle until I was satisfied with the quality of the model.\n\n![Figure 2. Predicted herbarium sheet, each colour is a separate leaf mask](Figure2.jpg){width=\"386\"}\n\n<p>\n\n \n\n<p>\n\n#### The second model, a leaf classifier model\n\nTo enhance the quality and accuracy of my workflow, I then developed a leaf classifier model. This model acted as an additional layer of filtration, classifying the output of the segmentation model into valid and invalid leaves.\n\nUsing the segmented leaf images generated by the first model, I manually annotated the images to create the necessary datasets for training, validating, and testing the leaf classifier model. Here, manual annotation involved me categorising leaf images generated by the first model as valid and invalid leaves. Then, like before, separating those images into the three datasets.\n\nOnce again, the creation of this model involved optimisation. Iterations of this machine learning model was crucial in ensuring the model was of apt quality and could accurately identify valid leaves.\n\n+:----------------------------------------------------------------:+:--------------------------------------------------------------------:+\n| ![Valid leaf](Valid%20leaf.jpg){fig-align=\"center\" width=\"183\"}\\ | ![Invalid leaf](Invalid%20leaf.jpg){fig-align=\"center\" width=\"269\"}\\ |\n| Valid leaf                                                       | Invalid leaf                                                         |\n+------------------------------------------------------------------+----------------------------------------------------------------------+\n\n: Figure 3. Example of valid and invalid leaf\n\n<p>\n\n \n\n<p>\n\n#### Applying the models to generate a massive dataset\n\nAfter being satisfied with both models, I applied them to the remaining herbarium images. This process generated a massive dataset, comprising of approximately 130,000 data points. The size of this dataset was 50 times larger than the previous datasets. This gave me an amazing framework where I could ask so many more questions during the analysis. It greatly expanded the sampling done at both a taxonomic level (inter- and intra- specific sampling), and at a spatial scale across Australia, to a degree not done before.\n\n<p>\n\n \n\n<p>\n\n#### Analysing the dataset\n\nThe final step in my project involved analysing the massive dataset created by the machine learning models. This dataset contained detailed information about the leaves, including their leaf area, leaf curvature and the size of the largest in-circle. As each herbarium sheet had their collection latitude and longitude I could then map it to climate, and see how the leaf traits changed across various variables. Furthermore, with their indepth taxonomic sampling, for the first time in literature, I explored how the leaf trait varied across the eucalypt taxonomy.\n\nFor a detailed analysis of the trends and patterns discovered in the dataset, check out my next blog post, where I share my novel findings and pretty graphs.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","editor":"visual","theme":"lux","title-block-banner":true,"title":"Generating and Analysing My Massive ML Data","date":"2023-04-24","categories":["code","project"],"image":"image.jpg"},"extensions":{"book":{"multiFile":true}}}}}